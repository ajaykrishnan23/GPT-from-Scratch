{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uQrfS4aub3d3"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class DummyGPTModel(nn.Module):\n",
        "\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_embedding = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "    self.pos_embedding = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.finalnorm = DummyLayerNorm(cfg['emb_dim'])\n",
        "    self.final_proj = nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
        "    self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    batch, seq_len = x.shape\n",
        "    device = x.device\n",
        "    x = self.tok_embedding(x)\n",
        "    x += self.pos_embedding(torch.arange(0,seq_len,device=x.device))\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.finalnorm(x)\n",
        "    x = self.final_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mtvCpaC3b5MY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "\n",
        "batch = torch.stack(batch,dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpXguSVBejaz",
        "outputId": "ad4e781e-5b4c-4e59-dee2-fb71cfb902ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjY1qxUIe8Lm",
        "outputId": "c6e52144-e06d-485e-cba7-46681604ca9b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6109, 3626, 6100,  345],\n",
              "        [6109, 1110, 6622,  257]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orade9Wze9lR",
        "outputId": "c884764d-b8aa-4fdc-df49-2022249ad967"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "eg = torch.randn(2,5)\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(eg)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vR3GVHsfBTy",
        "outputId": "c276c157-b426-4745-cc92-ec560ea16527"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# layer norm\n",
        "'''\n",
        "val - mean / sqrt(var)\n",
        "'''\n",
        "\n",
        "mean = out.mean(-1,keepdim=True)\n",
        "var = out.var(-1, keepdim=True)\n",
        "\n",
        "out_norm = (out - mean)/torch.sqrt(var)\n"
      ],
      "metadata": {
        "id": "D8_VXE37jX6b"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_norm.mean(-1,keepdim=True), out_norm.var(-1,keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWwM1qtrkC3v",
        "outputId": "2cbb9c57-15a3-4ab9-b1f0-5d211e377bca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[9.9341e-09],\n",
              "         [1.9868e-08]], grad_fn=<MeanBackward1>),\n",
              " tensor([[1.0000],\n",
              "         [1.0000]], grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.weight = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.bias = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(-1,keepdim=True)\n",
        "    var = x.var(-1,keepdim=True, unbiased=False)\n",
        "\n",
        "    out_norm = (x - mean)/torch.sqrt(var + self.eps)\n",
        "    return self.weight  * out_norm + self.bias"
      ],
      "metadata": {
        "id": "iL8BJV7xkOmh"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(6)"
      ],
      "metadata": {
        "id": "l32VDZ3llWrO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_out = ln(out)\n",
        "normalized_out.mean(-1), normalized_out.var(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuXFyyjJlcOl",
        "outputId": "b5589638-47ff-48dd-e356-6c32f1deaa34"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0000e+00, -1.9868e-08], grad_fn=<MeanBackward1>),\n",
              " tensor([1.1994, 1.1996], grad_fn=<VarBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GeLU(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2)/torch.pi) * (x + 0.044715 * torch.pow(x,3))))"
      ],
      "metadata": {
        "id": "xZBsG-oslnyy"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(nn.Linear(cfg['emb_dim'],cfg['emb_dim']*4), GeLU(),\n",
        "                                nn.Linear(cfg['emb_dim']*4,cfg['emb_dim']))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "vTAfy7UQufU1"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768)\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I77PUtGvFwK",
        "outputId": "9f85b796-dc7a-4a8f-c060-8c9a813f8609"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self, din, dout, context_len, num_heads, dropout, qkv_bias):\n",
        "    super().__init__()\n",
        "\n",
        "    self.wq = nn.Linear(din,dout, bias=qkv_bias)\n",
        "    self.wk = nn.Linear(din,dout, bias=qkv_bias)\n",
        "    self.wv = nn.Linear(din,dout, bias=qkv_bias)\n",
        "\n",
        "    self.head_dim = dout // num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.register_buffer('mask',torch.tril(torch.ones(context_len,context_len).unsqueeze(0).unsqueeze(0)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out_proj = nn.Linear(dout, dout)\n",
        "    self.dout = dout\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    Q = self.wq(x).view(B,T,self.num_heads, self.head_dim).transpose(1,2)\n",
        "    K = self.wk(x).view(B,T,self.num_heads, self.head_dim).transpose(1,2)\n",
        "    V = self.wv(x).view(B,T,self.num_heads, self.head_dim).transpose(1,2)  # B, numtokens, din  -> B, numheads, T, headdim\n",
        "\n",
        "    attention_scores = Q @ K.transpose(2,3) # B numheads T T\n",
        "    attention_scores = attention_scores.masked_fill(self.mask[:,:,:T,:T]==0, -torch.inf) # B numheads T T\n",
        "    attention_scores = attention_scores / self.head_dim**0.5 # B numheads T T\n",
        "    attention_scores = torch.softmax(attention_scores, dim=-1) # B numheads T T\n",
        "    attention_scores = self.dropout(attention_scores)\n",
        "    context_vector = attention_scores @ V # B numheads T headdim\n",
        "    context_vector = context_vector.transpose(1,2) # B T numheads Headdim\n",
        "\n",
        "    context_vector = context_vector.contiguous().view(B,T,self.dout)\n",
        "    context_vector = self.out_proj(context_vector)\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "zdSePBVGxCFf"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "PyUBUX0jxbPa"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.ln1 = LayerNorm(cfg['emb_dim'])\n",
        "    self.mha = MultiHeadAttention(cfg['emb_dim'], cfg['emb_dim'], cfg['context_length'], cfg['n_heads'], cfg['drop_rate'], cfg['qkv_bias'])\n",
        "\n",
        "    self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    self.ln2 = LayerNorm(cfg['emb_dim'])\n",
        "    self.ffw = FeedForward(cfg)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.dropout(self.mha(self.ln1(x)))\n",
        "\n",
        "    x = x + self.dropout(self.ffw(self.ln2(x)))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "VRKHJChovKSj"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trnf = TransformerBlock(GPT_CONFIG_124M)"
      ],
      "metadata": {
        "id": "K2ACoyNlyTgg"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = trnf(torch.randn(2,4,768))"
      ],
      "metadata": {
        "id": "P_2FtBg0yWIQ"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCBlRiotyg6Y",
        "outputId": "1289b30a-155e-46f1-91eb-0f039ce4a935"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.tok_emb_layer = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
        "    self.pos_emb_layer = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
        "\n",
        "    self.dropout = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "    self.trfblocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
        "    )\n",
        "\n",
        "    self.final_ln = LayerNorm(cfg['emb_dim'])\n",
        "    self.out_proj = nn.Linear(cfg['emb_dim'],cfg['vocab_size'], bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    B,T = x.shape\n",
        "    x = self.tok_emb_layer(x) + self.pos_emb_layer(torch.arange(0,T,device=x.device))\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.trfblocks(x)\n",
        "\n",
        "    x = self.final_ln(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "CfirYSOlzch8"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPT2(GPT_CONFIG_124M)"
      ],
      "metadata": {
        "id": "N4pAkf894KJ5"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(batch)"
      ],
      "metadata": {
        "id": "uhpna3vt4p_0"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_25Hyen64sjA",
        "outputId": "3ea3c39d-5c77-438f-87a3-5e277d9f57d5"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum([p.nelement() for p in model.parameters()])\n"
      ],
      "metadata": {
        "id": "o1zMeGxD4uby"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb_layer.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_proj.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvDpUiZk44os",
        "outputId": "c12699e3-8394-4432-93a4-88b7e1718c42"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = (\n",
        "    sum([p.nelement() for p in model.parameters()]) - sum(p.numel()\n",
        "    for p in model.out_proj.parameters())\n",
        ")\n",
        "print(f\"Number of trainable parameters \"\n",
        "      f\"considering weight tying: {total_params_gpt2:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIAjxhn45CRl",
        "outputId": "c80a3a14-39e0-40bd-b5e3-542c744fd7de"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 124,439,808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = (\n",
        "    sum([p.nelement() for p in model.parameters()]) - sum(p.numel()\n",
        "    for p in model.out_proj.parameters())\n",
        ")\n",
        "print(f\"Number of trainable parameters \"\n",
        "      f\"considering weight tying: {total_params_gpt2:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRb2-5JS5MHZ",
        "outputId": "7d89b408-ff4f-4a7f-8ecb-9a5139b68903"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 124,412,160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9a7kMwn5Rft",
        "outputId": "1e823b2b-add5-4f74-ee49-e4499dfe8776"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 621.83 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.trfblocks[0].mha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNRdZA4j6f7U",
        "outputId": "100fa17d-4863-45da-82ba-8adc62b1ab7f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (wq): Linear(in_features=768, out_features=768, bias=False)\n",
              "  (wk): Linear(in_features=768, out_features=768, bias=False)\n",
              "  (wv): Linear(in_features=768, out_features=768, bias=False)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_params(layer):\n",
        "  s = 0\n",
        "  for p in layer.parameters():\n",
        "    s += p.numel()\n",
        "  return s\n",
        "\n",
        "calc_params(model.trfblocks[0].ffw) * 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTq069HM7jH5",
        "outputId": "3725969b-ca07-478e-8e77-6e0959d927d1"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56669184"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_params(model.trfblocks[0].mha) * 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtVGd0kq7_BY",
        "outputId": "4360540c-e28b-4e0b-bd2d-9d9729b743c7"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28320768"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = [1,2,3,4,5]\n",
        "a[-2:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4vdruaE9wNN",
        "outputId": "20cf2be1-d8d9-4867-94ce-bcd8d2b08726"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "  # idx: batch,numtokens\n",
        "  model.eval()\n",
        "  for _ in range(max_new_tokens):\n",
        "    context_to_give = idx[:,-context_size:]\n",
        "    # with torch.no_grad():\n",
        "    output = model(context_to_give) # 1,1,50k\n",
        "    output = torch.softmax(output,dim=-1)\n",
        "    # return output\n",
        "    next_idx = torch.multinomial(output[0,0,:],1)\n",
        "    idx = torch.cat([idx,next_idx.view(1,1)],dim=-1)\n",
        "\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "VLp1OrZ68Lzo"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Ajay is chilling\"\n",
        "encoded_text = tokenizer.encode(input_text)\n",
        "len(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg3yovw7BOYU",
        "outputId": "876a9d45-596f-4f66-f316-b80878b58649"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_tokens = generate_text_simple(model, torch.tensor(encoded_text).view(1,-1), 10, GPT_CONFIG_124M['context_length'])"
      ],
      "metadata": {
        "id": "m_luWaUb-qkI"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(generated_tokens.squeeze(0).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9NKJjtts_EX6",
        "outputId": "4de7d959-0835-4b7c-88a7-d82003712b21"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ajay is chilling stay Trouble killed EXTmaybe secretiveHow GiulianiStudHealth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKTwlcvFAroX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx,\n",
        "                         max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "cF3K5WtH_Tg5"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgMlAKC_ANpo",
        "outputId": "9f1e68be-7263-43c1-c8ce-9a9e414273ce"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=encoded_tensor,\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ3DRS6HAQaj",
        "outputId": "9c77a84a-02b0-4c7e-d9be-26d7bb4df4bd"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
            "         49706, 43231, 47062, 34657]])\n",
            "Output length: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObNXzIh5BCWC",
        "outputId": "7255a0e4-5d56-4b4e-d458-dcfc7bb1939f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fbiYHET7BEIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
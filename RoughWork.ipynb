{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NYKXsIEQ3IM",
        "outputId": "882bba2c-ec75-4136-fd7c-7e2b61c9918e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-21 19:32:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-05-21 19:32:14 (90.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We\" always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9DJVAJjwQ6PR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFsM01mEQ-Uf",
        "outputId": "41539926-ab40-4d21-ea42-ec89e46085ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbmsR-05RABs",
        "outputId": "7d07af96-bd81-4595-8897-0ccdc3c3e923"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting full vocab\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Vocabulary: {''.join(chars)}, Vocab Size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1iUUQZkRAtT",
        "outputId": "66fd19ca-77f6-4f26-ff10-f32c10e05781"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz, Vocab Size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the mapping\n",
        "itos = {i:s for i,s in enumerate(chars)}\n",
        "stoi = {s:i for i,s in itos.items()}\n",
        "\n",
        "encode = lambda l: [stoi[c] for c in l]\n",
        "decode = lambda l: \"\".join([itos[c] for c in l])\n",
        "\n",
        "\n",
        "print(encode('hii there'))\n",
        "print(decode(encode('hii there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsqoljzTRNHz",
        "outputId": "aeace31f-a811-495c-adb1-2b38055fc129"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use pytorch to access and store it\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyBaWPdqR0xH",
        "outputId": "fa051d4d-1c90-4128-c36e-54b02aa46941"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data into train and test\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "qpcbFLP7R4ts"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1] # we gotta do block size + 1 cuz we'll use 8 chars as context to genrate the 9th one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od3ZhCmVSJG8",
        "outputId": "7430392b-0a81-402f-9fba-0cf1c421f27b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb = train_data[:block_size + 1]\n",
        "yb = train_data[1:block_size + 1]\n",
        "\n",
        "\n",
        "for t in range(block_size):\n",
        "  print(\"Input\", xb[:t+1])\n",
        "  print(\"Output Given input\", yb[t])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgLGOo70SN3b",
        "outputId": "9440a944-1802-49ac-feb7-e68a5d70948d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor([18])\n",
            "Output Given input tensor(47)\n",
            "Input tensor([18, 47])\n",
            "Output Given input tensor(56)\n",
            "Input tensor([18, 47, 56])\n",
            "Output Given input tensor(57)\n",
            "Input tensor([18, 47, 56, 57])\n",
            "Output Given input tensor(58)\n",
            "Input tensor([18, 47, 56, 57, 58])\n",
            "Output Given input tensor(1)\n",
            "Input tensor([18, 47, 56, 57, 58,  1])\n",
            "Output Given input tensor(15)\n",
            "Input tensor([18, 47, 56, 57, 58,  1, 15])\n",
            "Output Given input tensor(47)\n",
            "Input tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
            "Output Given input tensor(58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "\n",
        "def get_batch(split):\n",
        "\n",
        "  data = train_data if split=='train' else val_data\n",
        "  ix = torch.randint(0,len(data)-block_size, (batch_size,))\n",
        "  xb = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  yb = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "\n",
        "  return xb, yb\n",
        "\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('outputs')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "\n",
        "for b in range(batch_size):\n",
        "\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "\n",
        "    print(f\"Context: {context.tolist()}, target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsYQl5XSSqb2",
        "outputId": "9c1884df-9ae2-4052-8a69-8b233d8f6537"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "outputs\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "Context: [24], target: 43\n",
            "Context: [24, 43], target: 58\n",
            "Context: [24, 43, 58], target: 5\n",
            "Context: [24, 43, 58, 5], target: 57\n",
            "Context: [24, 43, 58, 5, 57], target: 1\n",
            "Context: [24, 43, 58, 5, 57, 1], target: 46\n",
            "Context: [24, 43, 58, 5, 57, 1, 46], target: 43\n",
            "Context: [24, 43, 58, 5, 57, 1, 46, 43], target: 39\n",
            "Context: [44], target: 53\n",
            "Context: [44, 53], target: 56\n",
            "Context: [44, 53, 56], target: 1\n",
            "Context: [44, 53, 56, 1], target: 58\n",
            "Context: [44, 53, 56, 1, 58], target: 46\n",
            "Context: [44, 53, 56, 1, 58, 46], target: 39\n",
            "Context: [44, 53, 56, 1, 58, 46, 39], target: 58\n",
            "Context: [44, 53, 56, 1, 58, 46, 39, 58], target: 1\n",
            "Context: [52], target: 58\n",
            "Context: [52, 58], target: 1\n",
            "Context: [52, 58, 1], target: 58\n",
            "Context: [52, 58, 1, 58], target: 46\n",
            "Context: [52, 58, 1, 58, 46], target: 39\n",
            "Context: [52, 58, 1, 58, 46, 39], target: 58\n",
            "Context: [52, 58, 1, 58, 46, 39, 58], target: 1\n",
            "Context: [52, 58, 1, 58, 46, 39, 58, 1], target: 46\n",
            "Context: [25], target: 17\n",
            "Context: [25, 17], target: 27\n",
            "Context: [25, 17, 27], target: 10\n",
            "Context: [25, 17, 27, 10], target: 0\n",
            "Context: [25, 17, 27, 10, 0], target: 21\n",
            "Context: [25, 17, 27, 10, 0, 21], target: 1\n",
            "Context: [25, 17, 27, 10, 0, 21, 1], target: 54\n",
            "Context: [25, 17, 27, 10, 0, 21, 1, 54], target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline: Bigram Language Model"
      ],
      "metadata": {
        "id": "Au53qH7nXQY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, target=None):\n",
        "\n",
        "    logits = self.embedding_table(idx)\n",
        "    B,T,C = logits.shape\n",
        "\n",
        "    if target is not None:\n",
        "      logits = logits.view(B*T,C)\n",
        "      target = target.view(B*T,)\n",
        "      loss = F.cross_entropy(logits, target)\n",
        "    else:\n",
        "      loss = None\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_num_tokens):\n",
        "    for _ in range(max_num_tokens):\n",
        "      logits, loss = self(idx) # B x T x C\n",
        "      # print(logits.shape)\n",
        "      logits = logits[:, -1, :] # take last layer dim alone\n",
        "      probs = F.softmax(logits, dim=-1) # column wise for each batch\n",
        "\n",
        "      idx_new = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_new), dim=1) # concat along dim 1, so it becomes Bx T+1\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape, loss)\n",
        "generated_result = m.generate(torch.zeros((1,1),dtype=torch.long), 100)\n",
        "print(decode(generated_result[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxa6j_ayTxV-",
        "outputId": "3714c38a-c096-49ca-8900-7477f7172600"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65]) tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((1,1),dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jt-1fPCxsdR",
        "outputId": "8e90c691-4fb6-42ba-9bb4-850ade53f28f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klUpoJSUwVUF",
        "outputId": "8dbc73d6-bab8-45b5-8b71-bab32a17e193"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 65]), tensor(4.8786, grad_fn=<NllLossBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "emb_out = emb(xb)\n",
        "\n",
        "print(xb.shape, emb_out.shape)\n",
        "\n",
        "\n",
        "\n",
        "emb_out[:,-1,:] # assumption is that you're using the last word to generate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5269lgTbeGg",
        "outputId": "1a2e70eb-4619-479e-b6f3-743f147588f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = emb_out.shape\n",
        "\n",
        "yb.view(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thSnFdo_Zcvj",
        "outputId": "4e535c11-95fc-468f-b421-f37660853e93"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 58,  5, 57,  1, 46, 43, 39, 53, 56,  1, 58, 46, 39, 58,  1, 58,  1,\n",
              "        58, 46, 39, 58,  1, 46, 17, 27, 10,  0, 21,  1, 54, 39])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_out.view(B*T, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1GyI_rMZgEK",
        "outputId": "3bde4062-3198-4d41-9682-082c61b20d0f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
              "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
              "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
              "        ...,\n",
              "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
              "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
              "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yb.view(B*T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HCOznU7aDSk",
        "outputId": "cb6115bd-a23b-4862-ce94-db7cbec8cb16"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([43, 58,  5, 57,  1, 46, 43, 39, 53, 56,  1, 58, 46, 39, 58,  1, 58,  1,\n",
              "        58, 46, 39, 58,  1, 46, 17, 27, 10,  0, 21,  1, 54, 39])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the bigram model\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "8IoVBIKbaiHj"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for _ in range(10000):\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-JJBR2z0saX",
        "outputId": "18bcebf3-4be3-4c6e-9b69-18efca7495b8"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4210023880004883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_num_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACwvoPsI1ImF",
        "outputId": "cc805bb2-a583-478e-dc22-1b7128269560"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Thaiby Lulfaseabot alt mpw thar, d is cthe parn gers b, ace t d t tllined y sss u dst hothin h.\n",
            "An,\n",
            "\n",
            "QUThiroligmushielowilit?-f s aishe se achyano OUST:\n",
            "Juy, wn pr ber,\n",
            "KE w r wourw. thate ma ore,\n",
            "TOpacedairar me hileof rrut pit; chin! aire ur ye yon outhe hed;\n",
            "Mut,\n",
            "S:\n",
            "Iforoupem blf ad s thoomat I:\n",
            "Chthe!\n",
            "A theamurswor RD anouponts vear he erm y s heasineisinel; wevin:\n",
            "Th s y sthis:\n",
            "G hestist, oriturea,\n",
            "Whes,\n",
            "\n",
            "Agallare bes T:\n",
            "Toiniseinllldet y, perekeil fus m aghaked sikie, ke angupes bee tisha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention: Intuition"
      ],
      "metadata": {
        "id": "lJBiPsDevmw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are we building?\n",
        "Let's understand that first. First we want the tokens to communicate with one anohter. Currently they aren't.\n",
        "\n",
        "How?\n",
        "\n",
        "Most naive example is an average.\n",
        "But, we only want present tokens interacting with past, not with future. how do we do that?"
      ],
      "metadata": {
        "id": "1F0ksK0dv3P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "03KOpeCB1EBI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive way\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,3\n",
        "\n",
        "x = torch.randn((B,T,C))\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1] # T,C\n",
        "    xbow[b,t] = xprev.mean(axis=0)\n",
        ""
      ],
      "metadata": {
        "id": "MIS3YUcm13Nm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# improve on this\n",
        "'''\n",
        "Thats a lot of for loops.\n",
        "We can optimize this. Let's do it on a matrix instead\n",
        "'''\n",
        "xtril = torch.tril(torch.ones(T,T))\n",
        "xtril /= torch.sum(xtril,dim=1,keepdim=True)\n",
        "\n",
        "output = xtril @ x\n",
        "torch.allclose(output,xbow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIxhe1wexWFv",
        "outputId": "15a4fdb1-ae4f-4bc7-def8-d9bd467516c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# improve on this further\n",
        "head_size = 16\n",
        "Q = nn.Linear(C, head_size)\n",
        "K = nn.Linear(C, head_size)\n",
        "V = nn.Linear(C, head_size)\n",
        "\n",
        "q = Q(x) # B T hs\n",
        "k = K(x) # B T hs\n",
        "\n",
        "wei = q @ k.transpose(-2,-1)\n",
        "\n",
        "xtril = torch.tril(torch.ones(T,T))\n",
        "wei = F.softmax(wei.masked_fill(xtril==0, value=float('-inf')) * head_size**0.5,dim=-1) # we divide by sqrt(head_size) to normalize. If we don't the values will look peaky\n",
        "\n",
        "v = V(x) # B T HS\n",
        "xbow = wei @ v\n",
        "# torch.allclose(output,output2)\n"
      ],
      "metadata": {
        "id": "HzNgyRU5zIZo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3]),dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6UgC9NOdNXV",
        "outputId": "f888c034-858f-45af-b595-7abdd840f87d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3376, 0.2501, 0.4123])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3])*8,dim=-1) # here if you see its sooo sharp towards the third option"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQAMncwwet2f",
        "outputId": "03931bec-77a1-49bd-8f96-db1178d5d58a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1655, 0.0150, 0.8195])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, n_embd, head_size, block_size, num_heads):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([SelfAttention(n_embd, head_size, block_size) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.cat([h(x) for h in self.heads],dim=-1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, block_size):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.k = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.v = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.hs = head_size\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, token_emb):\n",
        "        B,T,C = token_emb.shape\n",
        "        q = self.q(token_emb) # B T HS\n",
        "        k = self.k(token_emb) # B T HS\n",
        "        v = self.v(token_emb) # B T HS\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) # B T HS @ B HS T\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) * self.hs ** 0.5\n",
        "        wei = F.softmax(wei,dim=-1) # B T T\n",
        "        xbow = wei @ v # B T T @ B T HS\n",
        "        return xbow # B T HS\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, n_embd, block_size, head_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    # self.sa_head = SelfAttention(n_embd, head_size, block_size)\n",
        "    self.sa_heads = MultiHeadAttention(n_embd, head_size//num_heads, block_size, num_heads)\n",
        "    self.lm_head = nn.Linear(head_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, target=None):\n",
        "    B,T = idx.shape\n",
        "    token_emb = self.token_embedding_table(idx) #B,T,n_embd\n",
        "    positional_output = self.position_embedding_table(torch.tensor(torch.arange(0,T),device=device)) #T,n_embd\n",
        "    x = token_emb + positional_output # B T n_embd\n",
        "    x = self.sa_heads(x) # B, T, HS\n",
        "    logits = self.lm_head(x) # B,T,vocab_size\n",
        "\n",
        "\n",
        "    if target is not None:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      target = target.view(B*T,)\n",
        "      loss = F.cross_entropy(logits, target)\n",
        "    else:\n",
        "      loss = None\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_num_tokens):\n",
        "    for i in range(max_num_tokens):\n",
        "      idx_needed = idx[:, -block_size:]\n",
        "      logits, loss = self(idx_needed) # B x T x C\n",
        "      # print(logits.shape)\n",
        "      logits = logits[:, -1, :] # take last layer dim alone\n",
        "      probs = F.softmax(logits, dim=-1) # column wise for each batch\n",
        "\n",
        "      idx_new = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_new), dim=1) # concat along dim 1, so it becomes Bx T+1\n",
        "    return idx\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model):\n",
        "    '''\n",
        "    get train loss, val loss\n",
        "    '''\n",
        "    model.eval()\n",
        "    out_dict = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            xb,yb = get_batch(split)\n",
        "            logits, loss = model(xb,yb)\n",
        "            losses[k] = loss.item()\n",
        "        out_dict[f'{split}_loss'] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "eval_iters = 200\n",
        "batch_size = 32\n",
        "eval_interval = 300\n",
        "max_steps = 10000\n",
        "n_embd = 32\n",
        "block_size = 8\n",
        "head_size = 16\n",
        "num_heads = 4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Detected Device\", device)\n",
        "\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size, n_embd, block_size, head_size).to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELUVyYN7eybg",
        "outputId": "14194f50-5da6-4685-e922-a6aec690ec55"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = m(torch.randint(0,vocab_size,(1,1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fOJDJpAiu45",
        "outputId": "c0fde783-e775-4be5-f1ed-3c53a83e6ea0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-383090e27e7c>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  positional_output = self.position_embedding_table(torch.tensor(torch.arange(0,T),device=device)) #T,n_embd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el9_-YMnjRKY",
        "outputId": "1c368b1c-92c4-4a49-cc03-e2d837395524"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_result = m.generate(torch.zeros((1,1),dtype=torch.long), 1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3AYYhIhjzgg",
        "outputId": "1312acf3-b929-4293-dc1c-20cb2f7a91b1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-383090e27e7c>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  positional_output = self.position_embedding_table(torch.tensor(torch.arange(0,T),device=device)) #T,n_embd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(generated_result[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AvK0Gafmzhj",
        "outputId": "5d0c0c53-1fbb-44bb-f2b7-2fa1b376a06e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "tDrK-k;E;:MZflJNcytT&-wBPfMZRLNcNBaabFdFAytVxFyFU:fGikXDRSLTbUPL?$La.'jH;\n",
            "kWKiyHzkmGsoCZg&auYkLsPNae'eC.iXN!pwJI-kC'j$OPdu!RafvAr.jUCivH$.WFEhgC?t:W'PBuGz;3Cf&;&tkXo$T.sTAFj.e; rCYh3,O.ztffkK.vEa\n",
            "CMmPXcn?qeCkkVfMXnFF inUEmZvKZWmYEX:fcydFuFwC.3cnp'&fSTHrbfciCBofpypsfeYM?:MmtyzMrJnVXcODsGujnLNp'?cmZHu3YvFe.rshrIH,J,Z!ri,All$aS\n",
            ";oukLtFBYz:aPe;e$Lu$aZeSpSncIbWYNmmgEqsXxBaJ!HPR$ApDUyQ:QnXjg\n",
            "CRAua&.kyxuV;zY: Cb;sRNZF':I\n",
            "EF;dKtK& '?,QePiFWm\n",
            "UhXpmhrrvssn&:.qiajsAMliNiYPbDKFe.!d?JTHEMQsnkNf FsU dBcWxmgsY?VhSRc\n",
            ".-lhP?pCs ,pZxs\n",
            "CNW&aSKKeq:nhVpdN3,FR'piiGVwyxo3$VwacF$fWjoVBxNVrkgRzOGC!vGrrzm\n",
            "gDLY.L Nsbc.3rMi$f\n",
            "-F &.bBsQLqUbXUyAqja!hQw-.;TQeYq-KnjR$aMbkGG'h-jGFn3BoHFCXac\n",
            "RfuSdJ$UJmuDsfkkWAgx.i-qqMPF hfJkwiETnOnuqpDgpy?grMiWGeREdMm.duYxNKhUEV?uD$UDq\n",
            "YTRjX;3I$ hVPZNPVnCxqzmSlnYeWTM3nuHiPXbGexA'OSnvBSxLG saBaV-Ro3d-NrbBaxkq$jNgST3NFgVq$ u!eY&GG$SxHRlkSbK-\n",
            "terOlos;eShueGHK?3SSAlI\n",
            "Uo\n",
            "eRWtEPtxE$Zi.H!CMfHmaXbdeFY;.YiGBY-HnfedbK NB?&yY\n",
            "VTfYDhq?z&ypiGm.P&X,B,uk-DXnGszhb3BaeL!EDk-k\n",
            "CQEAuH3iSDQmqbAoBgp$Zr!j!.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1d(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, momentum=1e-3, eps=1e-8, training=True):\n",
        "\n",
        "    self.gamma = torch.ones((1,n_embd))\n",
        "    self.beta = torch.zeros((1,n_embd))\n",
        "\n",
        "    self.running_mean = torch.zeros((1,n_embd))\n",
        "    self.running_var = torch.ones((1,n_embd))\n",
        "    self.training = training\n",
        "    self.eps = eps\n",
        "    self.momentum = momentum\n",
        "\n",
        "  def __call__(self,x):\n",
        "    # x size - B,C\n",
        "    if not self.training:\n",
        "      xmean = self.running_mean\n",
        "      xvar = self.running_var\n",
        "\n",
        "    else:\n",
        "      xmean = x.mean(0,keepdim=True) # 1,C\n",
        "      xvar = x.var(0,keepdim=True) # 1,C\n",
        "\n",
        "    x_bar = (x-xmean)/torch.sqrt(xvar+self.eps) # 1,c\n",
        "    x_bar = self.gamma * x_bar + self.beta # 1, c\n",
        "    if self.training:\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = self.momentum * xmean + (1-self.momentum) * self.running_mean\n",
        "        self.running_var = self.momentum * xmean + (1-self.momentum) * self.running_var\n",
        "\n",
        "    return x_bar\n",
        "\n"
      ],
      "metadata": {
        "id": "qDfnAtlaoB_q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn = BatchNorm1d(n_embd)\n",
        "x = torch.randn(4,n_embd)"
      ],
      "metadata": {
        "id": "hi8fgv3SXphg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = bn(x)"
      ],
      "metadata": {
        "id": "OGp-kkt5XwzC"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out[:,0].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3nNtplnX0H8",
        "outputId": "d6ad4330-b873-482a-ec73-c8964e57dd10"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[:,2].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9wa8_qHX0tq",
        "outputId": "aa13f94c-f714-401c-a453-d314f1dae391"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm:\n",
        "\n",
        "  def __init__(self, n_embd, eps=1e-8):\n",
        "\n",
        "    self.gamma = torch.ones((1,n_embd))\n",
        "    self.beta = torch.zeros((1,n_embd))\n",
        "    self.eps = eps\n",
        "\n",
        "  def __call__(self,x):\n",
        "    # x size - B,C\n",
        "\n",
        "    xmean = x.mean(1,keepdim=True) # B,1\n",
        "    xvar = x.var(1,keepdim=True) # B,1\n",
        "\n",
        "    x_bar = (x-xmean)/torch.sqrt(xvar+self.eps) # B,C\n",
        "    x_bar = self.gamma * x_bar + self.beta # B,C\n",
        "\n",
        "    return x_bar\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma + self.beta]\n",
        "\n"
      ],
      "metadata": {
        "id": "mbW01ouwX7NZ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4,n_embd)\n",
        "ln = LayerNorm(n_embd)\n",
        "out = ln(x)\n",
        "out[2,:].mean(), out[2,:].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQrraGhEZbep",
        "outputId": "bb833e90-ef56-49be-e2f2-bff4d24bba57"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-1.1176e-08), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MzCDWat_Zgnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}